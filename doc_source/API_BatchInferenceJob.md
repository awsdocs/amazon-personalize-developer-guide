# BatchInferenceJob<a name="API_BatchInferenceJob"></a>

Contains information on a batch inference job\.

## Contents<a name="API_BatchInferenceJob_Contents"></a>

 ** batchInferenceJobArn **   <a name="personalize-Type-BatchInferenceJob-batchInferenceJobArn"></a>
The Amazon Resource Name \(ARN\) of the batch inference job\.  
Type: String  
Length Constraints: Maximum length of 256\.  
Pattern: `arn:([a-z\d-]+):personalize:.*:.*:.+`   
Required: No

 ** batchInferenceJobConfig **   <a name="personalize-Type-BatchInferenceJob-batchInferenceJobConfig"></a>
A string to string map of the configuration details of a batch inference job\.  
Type: [BatchInferenceJobConfig](API_BatchInferenceJobConfig.md) object  
Required: No

 ** creationDateTime **   <a name="personalize-Type-BatchInferenceJob-creationDateTime"></a>
The time at which the batch inference job was created\.  
Type: Timestamp  
Required: No

 ** failureReason **   <a name="personalize-Type-BatchInferenceJob-failureReason"></a>
If the batch inference job failed, the reason for the failure\.  
Type: String  
Required: No

 ** filterArn **   <a name="personalize-Type-BatchInferenceJob-filterArn"></a>
The ARN of the filter used on the batch inference job\.  
Type: String  
Length Constraints: Maximum length of 256\.  
Pattern: `arn:([a-z\d-]+):personalize:.*:.*:.+`   
Required: No

 ** jobInput **   <a name="personalize-Type-BatchInferenceJob-jobInput"></a>
The Amazon S3 path that leads to the input data used to generate the batch inference job\.  
Type: [BatchInferenceJobInput](API_BatchInferenceJobInput.md) object  
Required: No

 ** jobName **   <a name="personalize-Type-BatchInferenceJob-jobName"></a>
The name of the batch inference job\.  
Type: String  
Length Constraints: Minimum length of 1\. Maximum length of 63\.  
Pattern: `^[a-zA-Z0-9][a-zA-Z0-9\-_]*`   
Required: No

 ** jobOutput **   <a name="personalize-Type-BatchInferenceJob-jobOutput"></a>
The Amazon S3 bucket that contains the output data generated by the batch inference job\.  
Type: [BatchInferenceJobOutput](API_BatchInferenceJobOutput.md) object  
Required: No

 ** lastUpdatedDateTime **   <a name="personalize-Type-BatchInferenceJob-lastUpdatedDateTime"></a>
The time at which the batch inference job was last updated\.  
Type: Timestamp  
Required: No

 ** numResults **   <a name="personalize-Type-BatchInferenceJob-numResults"></a>
The number of recommendations generated by the batch inference job\. This number includes the error messages generated for failed input records\.  
Type: Integer  
Required: No

 ** roleArn **   <a name="personalize-Type-BatchInferenceJob-roleArn"></a>
The ARN of the Amazon Identity and Access Management \(IAM\) role that requested the batch inference job\.  
Type: String  
Length Constraints: Maximum length of 256\.  
Pattern: `arn:([a-z\d-]+):iam::\d{12}:role/?[a-zA-Z_0-9+=,.@\-_/]+`   
Required: No

 ** solutionVersionArn **   <a name="personalize-Type-BatchInferenceJob-solutionVersionArn"></a>
The Amazon Resource Name \(ARN\) of the solution version from which the batch inference job was created\.  
Type: String  
Length Constraints: Maximum length of 256\.  
Pattern: `arn:([a-z\d-]+):personalize:.*:.*:.+`   
Required: No

 ** status **   <a name="personalize-Type-BatchInferenceJob-status"></a>
The status of the batch inference job\. The status is one of the following values:  
+ PENDING
+ IN PROGRESS
+ ACTIVE
+ CREATE FAILED
Type: String  
Length Constraints: Maximum length of 256\.  
Required: No

## See Also<a name="API_BatchInferenceJob_SeeAlso"></a>

For more information about using this API in one of the language\-specific AWS SDKs, see the following:
+  [ AWS SDK for C\+\+](https://docs.aws.amazon.com/goto/SdkForCpp/personalize-2018-05-22/BatchInferenceJob) 
+  [ AWS SDK for Go](https://docs.aws.amazon.com/goto/SdkForGoV1/personalize-2018-05-22/BatchInferenceJob) 
+  [ AWS SDK for Java V2](https://docs.aws.amazon.com/goto/SdkForJavaV2/personalize-2018-05-22/BatchInferenceJob) 
+  [ AWS SDK for Ruby V3](https://docs.aws.amazon.com/goto/SdkForRubyV3/personalize-2018-05-22/BatchInferenceJob) 